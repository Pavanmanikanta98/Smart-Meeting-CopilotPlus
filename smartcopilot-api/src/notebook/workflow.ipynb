{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Literal\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, Parallel, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# --- LLM & Config ---\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\") # pyright: ignore[reportCallIssue]\n",
    "\n",
    "# --- Data Storage (ID reference) ---\n",
    "class DataStorage:\n",
    "    _storage = { 'transcripts': {}, 'summaries': {}, 'insights': {}, 'action_items': {} }\n",
    "\n",
    "    @classmethod\n",
    "    def store(cls, kind: str, data: Any) -> str:\n",
    "        uid = f\"{kind}_{int(time.time()*1000)}\"\n",
    "        cls._storage[kind][uid] = data\n",
    "        return uid\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, kind: str, uid: str) -> Any:\n",
    "        return cls._storage[kind].get(uid)\n",
    "\n",
    "# --- Slack Tools (via MCP) ---\n",
    "@tool\n",
    "def slack_list_channels(limit: int = 100, cursor: Optional[str] = None) -> Dict:\n",
    "    return mcp_manager.call_tool(\"slack_list_channels\", {\"limit\": limit, \"cursor\": cursor})\n",
    "\n",
    "@tool\n",
    "def slack_post_message(channel_id: str, text: str) -> Dict:\n",
    "    if len(text) > 3900:\n",
    "        text = text[:3900] + \"...\"\n",
    "    return mcp_manager.call_tool(\"slack_post_message\", {\"channel_id\": channel_id, \"text\": text})\n",
    "\n",
    "@tool\n",
    "def slack_find_user_by_name(name: str) -> Optional[str]:\n",
    "    users = slack_list_channels(limit=200).get('members', []) # pyright: ignore[reportCallIssue]\n",
    "    for u in users:\n",
    "        if name.lower() in u.get('real_name', '').lower():\n",
    "            return u['id']\n",
    "    return None\n",
    "\n",
    "# --- Agent Prompts & Creation ---\n",
    "\n",
    "def make_react_agent(system_prompt: str, tools: List) -> Any:\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"inputs\")\n",
    "    ])\n",
    "    return create_react_agent(model=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "# 1) Summary Agent\n",
    "summary_prompt = \"\"\"\n",
    "You are MeetingPlus-SummaryAgent.\n",
    "- Input: transcript_id\n",
    "- Load transcript via DataStorage.load('transcripts', transcript_id)\n",
    "- Produce a concise bullet-point summary (<300 words).\n",
    "- Return JSON: {\"summary\": \"...\"}\n",
    "\"\"\"\n",
    "summary_agent = make_react_agent(summary_prompt, [])\n",
    "\n",
    "# 2) Insights Agent\n",
    "insights_prompt = \"\"\"\n",
    "You are MeetingPlus-InsightsAgent.\n",
    "- Input: transcript_id\n",
    "- Load transcript via DataStorage.load('transcripts', transcript_id)\n",
    "- Extract key insights as list of strings.\n",
    "- Identify mentioned participants by name.\n",
    "- Return JSON: {\"insights\": [...], \"users\": [\"Alice\", \"Bob\"]}\n",
    "\"\"\"\n",
    "insights_agent = make_react_agent(insights_prompt, [])\n",
    "\n",
    "# 3) Slack Dispatch Agent\n",
    "dispatch_prompt = \"\"\"\n",
    "You are MeetingPlus-SlackAgent, responsible for broadcasting.\n",
    "<Context>\n",
    "- Inputs: summary_id, insights_id, action_items_id, topic_tags, user_map\n",
    "- Tools: slack_list_channels, slack_post_message, slack_find_user_by_name\n",
    "</Context>\n",
    "<Responsibilities>\n",
    "1. Load via DataStorage.load\n",
    "2. Always post summary in 'all-abc'.\n",
    "3. For each topic in topic_tags, post in '{topic}-team' if exists.\n",
    "4. Format: *Summary*, *Insights*, *Action Items* with <@user_id> tags.\n",
    "5. Single thread per channel.\n",
    "6. Return JSON: {\"dispatched\": [{\"channel_id\":\"...\",\"ts\":\"...\"}], \"warnings\": []}\n",
    "</Responsibilities>\n",
    "<Error Handling>\n",
    "- Skip missing channels, warn.\n",
    "- Retry each failure once.\n",
    "</Error Handling>\n",
    "<Output>Valid JSON only.</Output>\n",
    "\"\"\"\n",
    "dispatch_agent = make_react_agent(dispatch_prompt, [slack_list_channels, slack_post_message, slack_find_user_by_name])\n",
    "\n",
    "@tool\n",
    "def slack_dispatch(\n",
    "    summary_id: str,\n",
    "    insights_id: str,\n",
    "    action_items_id: str,\n",
    "    topic_tags: List[str],\n",
    "    user_map: Dict[str, str]\n",
    ") -> Dict:\n",
    "    payload = {\"summary_id\": summary_id, \"insights_id\": insights_id,\n",
    "               \"action_items_id\": action_items_id,\n",
    "               \"topic_tags\": topic_tags, \"user_map\": user_map}\n",
    "    result = dispatch_agent.invoke({\"inputs\": payload})\n",
    "    return json.loads(result)\n",
    "\n",
    "# --- StateGraph Definition ---\n",
    "graph = StateGraph()\n",
    "\n",
    "# Coordinator: ingest transcript\n",
    "@graph.node()\n",
    "def coordinator(transcript: str) -> Dict[str, str]:\n",
    "    tid = DataStorage.store('transcripts', transcript)\n",
    "    return {'transcript_id': tid}\n",
    "\n",
    "# Parallel summary & insights\n",
    "parallel = Parallel(on_success='verify')\n",
    "\n",
    "@parallel.node()\n",
    "def summarization(transcript_id: str) -> Dict[str, str]:\n",
    "    out = summary_agent.invoke({'inputs': {'transcript_id': transcript_id}})\n",
    "    sid = DataStorage.store('summaries', json.loads(out)['summary'])\n",
    "    return {'summary_id': sid}\n",
    "\n",
    "@parallel.node()\n",
    "def insights(transcript_id: str) -> Dict[str, Any]:\n",
    "    out = insights_agent.invoke({'inputs': {'transcript_id': transcript_id}})\n",
    "    data = json.loads(out)\n",
    "    iid = DataStorage.store('insights', data['insights'])\n",
    "    # build user_map\n",
    "    return {'insights_id': iid, 'user_map': {u: None for u in data['users']},\n",
    "            'topic_tags': data.get('topics', [])}\n",
    "\n",
    "graph.add_parallel(parallel, requires=['transcript_id'])\n",
    "\n",
    "# Simple verification: ensure IDs exist\n",
    "@graph.node(on_success='dispatch', on_error='error')\n",
    "def verify(summary_id: str, insights_id: str) -> None:\n",
    "    assert DataStorage.load('summaries', summary_id)\n",
    "    assert DataStorage.load('insights', insights_id)\n",
    "    return {}\n",
    "\n",
    "# Dispatch to Slack\n",
    "@graph.node(on_success=END, on_error='error')\n",
    "def dispatch(\n",
    "    summary_id: str,\n",
    "    insights_id: str,\n",
    "    action_items_id: str,\n",
    "    topic_tags: List[str],\n",
    "    user_map: Dict[str, str]\n",
    ") -> Dict:\n",
    "    return slack_dispatch(summary_id, insights_id, action_items_id, topic_tags, user_map)\n",
    "\n",
    "# Error handler\n",
    "@graph.node()\n",
    "def error(**kwargs):\n",
    "    # log or notify ops\n",
    "    return {}\n",
    "\n",
    "# Entry point\n",
    "def run_pipeline(transcript: str):\n",
    "    return graph.run({'transcript': transcript})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8523edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82bdec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585e9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf65cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcef4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d24216f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 160\u001b[39m\n\u001b[32m    156\u001b[39m workflow.add_edge(\u001b[33m\"\u001b[39m\u001b[33mrun_insights_agent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdecide_next_action\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Compile the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m app = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# --- Example Usage ---\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Starting Workflow ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ds/pro/smart-meeting-copilot/smartcopilot-api/.venv/lib/python3.12/site-packages/langgraph/graph/state.py:618\u001b[39m, in \u001b[36mStateGraph.compile\u001b[39m\u001b[34m(self, checkpointer, cache, store, interrupt_before, interrupt_after, debug, name)\u001b[39m\n\u001b[32m    615\u001b[39m interrupt_after = interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    617\u001b[39m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[32m    627\u001b[39m output_channels = (\n\u001b[32m    628\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__root__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.schemas[\u001b[38;5;28mself\u001b[39m.output]) == \u001b[32m1\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    635\u001b[39m     ]\n\u001b[32m    636\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ds/pro/smart-meeting-copilot/smartcopilot-api/.venv/lib/python3.12/site-packages/langgraph/graph/graph.py:289\u001b[39m, in \u001b[36mGraph.validate\u001b[39m\u001b[34m(self, interrupt)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m branch.ends \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m end \u001b[38;5;129;01min\u001b[39;00m branch.ends.values():\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m end != END:\n\u001b[32m    290\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    291\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAt \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m node, \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcond\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m branch found unknown target \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m             )\n\u001b[32m    293\u001b[39m         all_targets.add(end)\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, END\n",
    "import random\n",
    "import time\n",
    "\n",
    "# 1. Define the Graph State\n",
    "# This state will be passed between nodes and updated.\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        summary_status (str): Current status of the summary agent ('pending', 'success', 'failed').\n",
    "        insights_status (str): Current status of the insights agent ('pending', 'success', 'failed').\n",
    "        summary_output (str): The output generated by the summary agent.\n",
    "        insights_output (str): The output generated by the insights agent.\n",
    "        iteration (int): Tracks the number of iterations/attempts.\n",
    "        error_message (str): Stores any general error message from the workflow.\n",
    "    \"\"\"\n",
    "    summary_status: str\n",
    "    insights_status: str\n",
    "    summary_output: str\n",
    "    insights_output: str\n",
    "    iteration: int\n",
    "    error_message: str\n",
    "\n",
    "# 2. Define the Agent Nodes (Placeholders)\n",
    "\n",
    "def run_summary_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Simulates the execution of the summary agent.\n",
    "    In a real scenario, this would call your LLM with a prompt for summarization.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Summary Agent (Iteration: {state['iteration']}) ---\")\n",
    "    new_state = state.copy()\n",
    "\n",
    "    # Simulate success/failure for demonstration\n",
    "    # For the first run, let's make it fail sometimes to test retry logic\n",
    "    # After the first run, make it succeed more often\n",
    "    if new_state['iteration'] == 1:\n",
    "        should_fail = random.choice([True, False]) # 50% chance to fail on first attempt\n",
    "    else:\n",
    "        should_fail = random.random() < 0.2 # 20% chance to fail on subsequent attempts\n",
    "\n",
    "    if should_fail:\n",
    "        new_state['summary_status'] = \"failed\"\n",
    "        new_state['summary_output'] = \"Summary generation failed due to an internal error.\"\n",
    "        print(\"Summary Agent: FAILED\")\n",
    "    else:\n",
    "        new_state['summary_status'] = \"success\"\n",
    "        new_state['summary_output'] = f\"This is a generated summary from iteration {new_state['iteration']}.\"\n",
    "        print(\"Summary Agent: SUCCESS\")\n",
    "\n",
    "    time.sleep(1) # Simulate work\n",
    "    return new_state\n",
    "\n",
    "def run_insights_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Simulates the execution of the insights agent.\n",
    "    In a real scenario, this would call your LLM with a prompt for insights.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Insights Agent (Iteration: {state['iteration']}) ---\")\n",
    "    new_state = state.copy()\n",
    "\n",
    "    # Simulate success/failure for demonstration\n",
    "    if new_state['iteration'] == 1:\n",
    "        should_fail = random.choice([True, False]) # 50% chance to fail on first attempt\n",
    "    else:\n",
    "        should_fail = random.random() < 0.2 # 20% chance to fail on subsequent attempts\n",
    "\n",
    "    if should_fail:\n",
    "        new_state['insights_status'] = \"failed\"\n",
    "        new_state['insights_output'] = \"Insights generation failed due to an API timeout.\"\n",
    "        print(\"Insights Agent: FAILED\")\n",
    "    else:\n",
    "        new_state['insights_status'] = \"success\"\n",
    "        new_state['insights_output'] = f\"These are the generated insights from iteration {new_state['iteration']}.\"\n",
    "        print(\"Insights Agent: SUCCESS\")\n",
    "\n",
    "    time.sleep(1) # Simulate work\n",
    "    return new_state\n",
    "\n",
    "# 3. Define the Supervisor Node (Reasoning and Decision-Making)\n",
    "def decide_next_action(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    The supervisor agent's reasoning capability.\n",
    "    It analyzes the current state and decides the next action.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Supervisor: Deciding Next Action (Iteration: {state['iteration']}) ---\")\n",
    "\n",
    "    summary_s = state['summary_status']\n",
    "    insights_s = state['insights_status']\n",
    "\n",
    "    # (1) If this is the first run, or both failed, call both in parallel\n",
    "    if (state['iteration'] == 0) or \\\n",
    "       (summary_s == \"failed\" and insights_s == \"failed\"):\n",
    "        print(\"Supervisor Decision: Initial run or both failed, calling both agents in parallel.\")\n",
    "        return \"call_both_parallel\"\n",
    "    # (2) If only the summary agent failed in previous run, call only the summary agent individually\n",
    "    elif summary_s == \"failed\" and insights_s == \"success\":\n",
    "        print(\"Supervisor Decision: Only summary failed, calling summary agent individually.\")\n",
    "        return \"call_summary_only\"\n",
    "    # (3) If only the insights agent failed, call only the insights agent individually\n",
    "    elif insights_s == \"failed\" and summary_s == \"success\":\n",
    "        print(\"Supervisor Decision: Only insights failed, calling insights agent individually.\")\n",
    "        return \"call_insights_only\"\n",
    "    # (4) If both succeeded, move to end state\n",
    "    elif summary_s == \"success\" and insights_s == \"success\":\n",
    "        print(\"Supervisor Decision: Both agents succeeded, workflow complete.\")\n",
    "        return \"end_workflow\"\n",
    "    else:\n",
    "        # This case should ideally not be reached with the current logic,\n",
    "        # but good for debugging or future expansions.\n",
    "        print(f\"Supervisor Decision: Unexpected state - Summary: {summary_s}, Insights: {insights_s}. Ending workflow.\")\n",
    "        return \"end_workflow\"\n",
    "\n",
    "# A node to increment the iteration count\n",
    "def increment_iteration(state: GraphState) -> GraphState:\n",
    "    new_state = state.copy()\n",
    "    new_state['iteration'] += 1\n",
    "    print(f\"\\n--- Incrementing Iteration to: {new_state['iteration']} ---\")\n",
    "    return new_state\n",
    "\n",
    "# 4. Construct the Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"increment_iteration\", increment_iteration)\n",
    "workflow.add_node(\"run_summary_agent\", run_summary_agent)\n",
    "workflow.add_node(\"run_insights_agent\", run_insights_agent)\n",
    "workflow.add_node(\"decide_next_action\", decide_next_action)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"increment_iteration\")\n",
    "\n",
    "# Define the edges (transitions)\n",
    "# After incrementing iteration, always decide next action\n",
    "workflow.add_edge(\"increment_iteration\", \"decide_next_action\")\n",
    "\n",
    "# Conditional routing from the supervisor's decision node\n",
    "workflow.add_conditional_edges(\n",
    "    \"decide_next_action\",\n",
    "    decide_next_action, # The function that determines the next edge\n",
    "    {\n",
    "        \"call_both_parallel\": [\"run_summary_agent\", \"run_insights_agent\"], # Call both in parallel\n",
    "        \"call_summary_only\": \"run_summary_agent\",\n",
    "        \"call_insights_only\": \"run_insights_agent\",\n",
    "        \"end_workflow\": END # End the graph if both succeeded\n",
    "    }\n",
    ")\n",
    "\n",
    "# After running summary agent (alone or in parallel), decide next action\n",
    "workflow.add_edge(\"run_summary_agent\", \"decide_next_action\")\n",
    "# After running insights agent (alone or in parallel), decide next action\n",
    "workflow.add_edge(\"run_insights_agent\", \"decide_next_action\")\n",
    "\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- Example Usage ---\n",
    "print(\"--- Starting Workflow ---\")\n",
    "\n",
    "# Initial state\n",
    "initial_state = {\n",
    "    \"summary_status\": \"pending\",\n",
    "    \"insights_status\": \"pending\",\n",
    "    \"summary_output\": \"\",\n",
    "    \"insights_output\": \"\",\n",
    "    \"iteration\": 0, # Start at 0, incremented to 1 on first run\n",
    "    \"error_message\": \"\"\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "# The graph will run until it reaches the END state.\n",
    "# You can uncomment the line below to view the graph structure\n",
    "# from IPython.display import Image, display\n",
    "# display(Image(app.get_graph().draw_png()))\n",
    "\n",
    "for s in app.stream(initial_state):\n",
    "    print(s)\n",
    "    print(\"--- Current State ---\")\n",
    "    # LangGraph streams the state updates. We can print the latest state.\n",
    "    for key, value in s.items():\n",
    "        if key != '__end__': # Don't print the internal __end__ key\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"---------------------\\n\")\n",
    "\n",
    "# Final state after execution\n",
    "final_state = next(iter(s.values())) # Get the last state yielded by the stream\n",
    "print(\"\\n--- Workflow Finished ---\")\n",
    "print(f\"Final Summary Status: {final_state['summary_status']}\")\n",
    "print(f\"Final Insights Status: {final_state['insights_status']}\")\n",
    "print(f\"Final Summary Output: {final_state['summary_output']}\")\n",
    "print(f\"Final Insights Output: {final_state['insights_output']}\")\n",
    "print(f\"Total Iterations: {final_state['iteration']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d771b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238cecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43d250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a1a79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/Ds/pro/smart-meeting-copilot/smartcopilot-api/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3672: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 241\u001b[39m\n\u001b[32m    237\u001b[39m workflow.add_edge(\u001b[33m\"\u001b[39m\u001b[33mrun_insights_agent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdecide_next_action\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# Compile the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m app = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# --- Example Usage ---\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Starting Workflow ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ds/pro/smart-meeting-copilot/smartcopilot-api/.venv/lib/python3.12/site-packages/langgraph/graph/state.py:618\u001b[39m, in \u001b[36mStateGraph.compile\u001b[39m\u001b[34m(self, checkpointer, cache, store, interrupt_before, interrupt_after, debug, name)\u001b[39m\n\u001b[32m    615\u001b[39m interrupt_after = interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    617\u001b[39m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[32m    627\u001b[39m output_channels = (\n\u001b[32m    628\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__root__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.schemas[\u001b[38;5;28mself\u001b[39m.output]) == \u001b[32m1\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    635\u001b[39m     ]\n\u001b[32m    636\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ds/pro/smart-meeting-copilot/smartcopilot-api/.venv/lib/python3.12/site-packages/langgraph/graph/graph.py:289\u001b[39m, in \u001b[36mGraph.validate\u001b[39m\u001b[34m(self, interrupt)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m branch.ends \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m end \u001b[38;5;129;01min\u001b[39;00m branch.ends.values():\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m end != END:\n\u001b[32m    290\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    291\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAt \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m node, \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcond\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m branch found unknown target \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m             )\n\u001b[32m    293\u001b[39m         all_targets.add(end)\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import TypedDict, Annotated, List, Literal\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, END\n",
    "import random\n",
    "import time\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# 1. Define the Graph State\n",
    "# This state will be passed between nodes and updated.\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        document_content (str): The raw meeting transaction content.\n",
    "        summary_status (str): Current status of the summary agent ('pending', 'success', 'failed').\n",
    "        insights_status (str): Current status of the insights agent ('pending', 'success', 'failed').\n",
    "        summary_output (str): The output generated by the summary agent.\n",
    "        insights_output (str): The output generated by the insights agent.\n",
    "        iteration (int): Tracks the number of iterations/attempts.\n",
    "        error_message (str): Stores any general error message from the workflow.\n",
    "        # Removed needs_confirmation, last_agent_output_for_confirmation, user_response\n",
    "        current_reasoning: str # To store the LLM's reasoning for its decision\n",
    "    \"\"\"\n",
    "    document_content: str\n",
    "    summary_status: str\n",
    "    insights_status: str\n",
    "    summary_output: str\n",
    "    insights_output: str\n",
    "    iteration: int\n",
    "    error_message: str\n",
    "    current_reasoning: str\n",
    "\n",
    "\n",
    "# Pydantic model for the LLM's structured output\n",
    "class Router(BaseModel):\n",
    "    \"\"\"Route the workflow to the next appropriate agent.\"\"\"\n",
    "    next: Literal[\"call_both_parallel\", \"call_summary_only\", \"call_insights_only\", \"end_workflow\"] = Field(\n",
    "        description=\"The name of the next node or action to route to.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"The reasoning behind the chosen route, based on agent statuses and workflow goals.\"\n",
    "    )\n",
    "\n",
    "# 2. Define the Agent Nodes (Placeholders)\n",
    "\n",
    "def run_summary_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Simulates the execution of the summary agent.\n",
    "    It uses the 'document_content' from the state as its input.\n",
    "    In a real scenario, this would call your LLM with a prompt for summarization\n",
    "    using state['document_content'].\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Summary Agent (Iteration: {state['iteration']}) ---\")\n",
    "    print(f\"Summary Agent processing content: '{state['document_content'][:50]}...'\")\n",
    "    new_state = state.copy()\n",
    "\n",
    "    # Simulate success/failure\n",
    "    if new_state['iteration'] == 1:\n",
    "        should_fail = random.choice([True, False]) # 50% chance to fail on first attempt\n",
    "    else:\n",
    "        should_fail = random.random() < 0.2 # 20% chance to fail on subsequent attempts\n",
    "\n",
    "    if should_fail:\n",
    "        new_state['summary_status'] = \"failed\"\n",
    "        new_state['summary_output'] = \"Summary generation failed due to an internal error.\"\n",
    "        print(\"Summary Agent: FAILED\")\n",
    "    else:\n",
    "        new_state['summary_status'] = \"success\"\n",
    "        new_state['summary_output'] = f\"This is a generated summary from iteration {new_state['iteration']} based on: {state['document_content'][:30]}...\"\n",
    "        print(\"Summary Agent: SUCCESS\")\n",
    "\n",
    "    time.sleep(1) # Simulate work\n",
    "    return new_state\n",
    "\n",
    "def run_insights_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Simulates the execution of the insights agent.\n",
    "    It uses the 'document_content' from the state as its input.\n",
    "    In a real scenario, this would call your LLM with a prompt for insights\n",
    "    using state['document_content'].\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Insights Agent (Iteration: {state['iteration']}) ---\")\n",
    "    print(f\"Insights Agent processing content: '{state['document_content'][:50]}...'\")\n",
    "    new_state = state.copy()\n",
    "\n",
    "    # Simulate success/failure\n",
    "    if new_state['iteration'] == 1:\n",
    "        should_fail = random.choice([True, False]) # 50% chance to fail on first attempt\n",
    "    else:\n",
    "        should_fail = random.random() < 0.2 # 20% chance to fail on subsequent attempts\n",
    "\n",
    "    if should_fail:\n",
    "        new_state['insights_status'] = \"failed\"\n",
    "        new_state['insights_output'] = \"Insights generation failed due to an API timeout.\"\n",
    "        print(\"Insights Agent: FAILED\")\n",
    "    else:\n",
    "        new_state['insights_status'] = \"success\"\n",
    "        new_state['insights_output'] = f\"These are the generated insights from iteration {new_state['iteration']} based on: {state['document_content'][:30]}...\"\n",
    "        print(\"Insights Agent: SUCCESS\")\n",
    "\n",
    "    time.sleep(1) # Simulate work\n",
    "    return new_state\n",
    "\n",
    "# 3. Define the Supervisor Node (Reasoning and Decision-Making with LLM)\n",
    "def decide_next_action(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    The supervisor agent's reasoning capability, driven by an LLM.\n",
    "    It analyzes the current state (summary/insights status) and decides the next action.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Supervisor: Deciding Next Action (Iteration: {state['iteration']}) ---\")\n",
    "    print(f\"Current Statuses: Summary={state['summary_status']}, Insights={state['insights_status']}\")\n",
    "\n",
    "    # System prompt for the supervisor LLM\n",
    "    system_prompt = \"\"\"\n",
    "    You are a highly intelligent supervisor agent tasked with managing a workflow to summarize a meeting transaction and extract key insights from it.\n",
    "    Your goal is to ensure both the 'summary_agent' and 'insights_agent' successfully complete their tasks.\n",
    "    You will receive the current status of these agents and must decide the next action.\n",
    "\n",
    "    ### AVAILABLE ACTIONS:\n",
    "    - 'call_both_parallel': Call both the summary and insights agents simultaneously. Use this for the initial run or if both agents previously failed.\n",
    "    - 'call_summary_only': Call only the summary agent. Use this if only the summary agent failed in the previous run, and the insights agent succeeded or was not run.\n",
    "    - 'call_insights_only': Call only the insights agent. Use this if only the insights agent failed in the previous run, and the summary agent succeeded or was not run.\n",
    "    - 'end_workflow': Terminate the workflow. Use this only when BOTH the summary agent and the insights agent have successfully completed their tasks.\n",
    "\n",
    "    ### CURRENT AGENT STATUSES:\n",
    "    - summary_status: {summary_status} (Can be 'pending', 'success', 'failed')\n",
    "    - insights_status: {insights_status} (Can be 'pending', 'success', 'failed')\n",
    "    - iteration: {iteration} (Indicates how many times the workflow has attempted actions)\n",
    "\n",
    "    ### RULES FOR DECISION-MAKING:\n",
    "    1.  **Initial Run:** If it's the very first attempt (iteration 1, and both statuses are 'pending'), always start by calling both agents in parallel.\n",
    "    2.  **Both Failed:** If both the summary_agent and insights_agent failed in their last attempt, retry both in parallel.\n",
    "    3.  **Summary Failed Only:** If only the summary_agent failed and the insights_agent succeeded, call only the summary_agent.\n",
    "    4.  **Insights Failed Only:** If only the insights_agent failed and the summary_agent succeeded, call only the insights_agent.\n",
    "    5.  **All Successful:** If both the summary_agent and insights_agent have a 'success' status, the workflow is complete. Route to 'end_workflow'.\n",
    "    6.  **Fallback/Unexpected State:** If the state doesn't perfectly match the above rules but the workflow is not yet complete (i.e., not both 'success'), default to retrying both in parallel to ensure progress. This acts as a safety net.\n",
    "\n",
    "    Your response MUST be a JSON object conforming to the 'Router' schema.\n",
    "    Provide clear and concise reasoning for your decision.\n",
    "    \"\"\"\n",
    "\n",
    "    # Simulate LLM call\n",
    "    # In a real scenario, you would use:\n",
    "    # from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    # groq_model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "    # response = groq_model.with_structured_output(Router).invoke(messages)\n",
    "\n",
    "    # For this example, we'll simulate the LLM's decision based on the rules.\n",
    "    # This simulates the LLM's reasoning based on the provided prompt.\n",
    "    simulated_llm_decision = Router(next=\"end_workflow\", reasoning=\"Default end.\") # Default\n",
    "\n",
    "    summary_s = state['summary_status']\n",
    "    insights_s = state['insights_status']\n",
    "    iteration = state['iteration']\n",
    "\n",
    "    # Rule 1 & 2: Initial run or both failed\n",
    "    if (iteration == 1 and summary_s == \"pending\" and insights_s == \"pending\") or \\\n",
    "       (summary_s == \"failed\" and insights_s == \"failed\"):\n",
    "        simulated_llm_decision = Router(\n",
    "            next=\"call_both_parallel\",\n",
    "            reasoning=\"Initial run or both agents failed, attempting parallel execution.\"\n",
    "        )\n",
    "    # Rule 3: Summary Failed Only\n",
    "    elif summary_s == \"failed\" and insights_s == \"success\":\n",
    "        simulated_llm_decision = Router(\n",
    "            next=\"call_summary_only\",\n",
    "            reasoning=\"Only the summary agent failed, retrying it individually.\"\n",
    "        )\n",
    "    # Rule 4: Insights Failed Only\n",
    "    elif insights_s == \"failed\" and summary_s == \"success\":\n",
    "        simulated_llm_decision = Router(\n",
    "            next=\"call_insights_only\",\n",
    "            reasoning=\"Only the insights agent failed, retrying it individually.\"\n",
    "        )\n",
    "    # Rule 5: All Successful\n",
    "    elif summary_s == \"success\" and insights_s == \"success\":\n",
    "        simulated_llm_decision = Router(\n",
    "            next=\"end_workflow\",\n",
    "            reasoning=\"Both summary and insights agents completed successfully. Workflow finished.\"\n",
    "        )\n",
    "    # Rule 6: Fallback/Unexpected State (Not yet successful)\n",
    "    else:\n",
    "        simulated_llm_decision = Router(\n",
    "            next=\"call_both_parallel\",\n",
    "            reasoning=\"Current state is ambiguous or not fully successful. Falling back to parallel retry to ensure progress.\"\n",
    "        )\n",
    "\n",
    "    print(f\"🤖 LLM Decision: {simulated_llm_decision.next} - Reasoning: {simulated_llm_decision.reasoning}\")\n",
    "\n",
    "    new_state = state.copy()\n",
    "    new_state['current_reasoning'] = simulated_llm_decision.reasoning\n",
    "\n",
    "    # Return the decision to LangGraph\n",
    "    return simulated_llm_decision.next if simulated_llm_decision.next != \"end_workflow\" else END\n",
    "\n",
    "\n",
    "# A node to increment the iteration count\n",
    "def increment_iteration(state: GraphState) -> GraphState:\n",
    "    new_state = state.copy()\n",
    "    new_state['iteration'] += 1\n",
    "    print(f\"\\n--- Incrementing Iteration to: {new_state['iteration']} ---\")\n",
    "    return new_state\n",
    "\n",
    "# 4. Construct the Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"increment_iteration\", increment_iteration)\n",
    "workflow.add_node(\"run_summary_agent\", run_summary_agent)\n",
    "workflow.add_node(\"run_insights_agent\", run_insights_agent)\n",
    "workflow.add_node(\"decide_next_action\", decide_next_action) # This is now LLM-driven\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"increment_iteration\")\n",
    "\n",
    "# Define the edges (transitions)\n",
    "# After incrementing iteration, always decide next action\n",
    "workflow.add_edge(\"increment_iteration\", \"decide_next_action\")\n",
    "\n",
    "# Conditional routing from the supervisor's decision node\n",
    "workflow.add_conditional_edges(\n",
    "    \"decide_next_action\",\n",
    "    decide_next_action, # The function that determines the next edge\n",
    "    {\n",
    "        \"call_both_parallel\": [\"run_summary_agent\", \"run_insights_agent\"], # Call both in parallel\n",
    "        \"call_summary_only\": \"run_summary_agent\",\n",
    "        \"call_insights_only\": \"run_insights_agent\",\n",
    "        \"end_workflow\": END # End the graph if both succeeded\n",
    "    }\n",
    ")\n",
    "\n",
    "# After running summary agent (alone or in parallel), decide next action\n",
    "workflow.add_edge(\"run_summary_agent\", \"decide_next_action\")\n",
    "# After running insights agent (alone or in parallel), decide next action\n",
    "workflow.add_edge(\"run_insights_agent\", \"decide_next_action\")\n",
    "\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- Example Usage ---\n",
    "print(\"--- Starting Workflow ---\")\n",
    "\n",
    "# Initial state for a meeting transaction\n",
    "initial_state = {\n",
    "    \"document_content\": \"Meeting minutes from Q3 planning session: Discussed budget allocations for marketing, R&D, and operations. Key decisions include increasing marketing spend by 15% for digital campaigns, freezing R&D hiring for Q4, and optimizing operational costs by 10% through vendor renegotiations. Action items: Marketing team to draft new campaign proposals by EOD Friday. Finance to provide updated budget forecasts by next Monday. Next meeting: October 15th.\",\n",
    "    \"summary_status\": \"pending\",\n",
    "    \"insights_status\": \"pending\",\n",
    "    \"summary_output\": \"\",\n",
    "    \"insights_output\": \"\",\n",
    "    \"iteration\": 0, # Start at 0, incremented to 1 on first run\n",
    "    \"error_message\": \"\",\n",
    "    \"current_reasoning\": \"\"\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "# The graph will run until it reaches the END state.\n",
    "# from IPython.display import Image, display\n",
    "# display(Image(app.get_graph().draw_png())) # Uncomment to visualize graph\n",
    "\n",
    "for s in app.stream(initial_state):\n",
    "    print(s)\n",
    "    print(\"--- Current State ---\")\n",
    "    # LangGraph streams the state updates. We can print the latest state.\n",
    "    for key, value in s.items():\n",
    "        if key != '__end__': # Don't print the internal __end__ key\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"---------------------\\n\")\n",
    "\n",
    "# Final state after execution\n",
    "final_state = next(iter(s.values())) # Get the last state yielded by the stream\n",
    "print(\"\\n--- Workflow Finished ---\")\n",
    "print(f\"Final Summary Status: {final_state['summary_status']}\")\n",
    "print(f\"Final Insights Status: {final_state['insights_status']}\")\n",
    "print(f\"Final Summary Output: {final_state['summary_output']}\")\n",
    "print(f\"Final Insights Output: {final_state['insights_output']}\")\n",
    "print(f\"Total Iterations: {final_state['iteration']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e1178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dac3d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 285\u001b[39m\n\u001b[32m    282\u001b[39m workflow.add_edge(\u001b[33m\"\u001b[39m\u001b[33mrun_insights_agent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mintelligent_supervisor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Compile\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m app = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# Test the intelligent workflow\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 Starting Intelligent Meeting Processing Workflow\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ds/pro/smart-meeting-copilot/smartcopilot-api/.venv/lib/python3.12/site-packages/langgraph/graph/state.py:618\u001b[39m, in \u001b[36mStateGraph.compile\u001b[39m\u001b[34m(self, checkpointer, cache, store, interrupt_before, interrupt_after, debug, name)\u001b[39m\n\u001b[32m    615\u001b[39m interrupt_after = interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    617\u001b[39m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[32m    627\u001b[39m output_channels = (\n\u001b[32m    628\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__root__\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.schemas[\u001b[38;5;28mself\u001b[39m.output]) == \u001b[32m1\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    635\u001b[39m     ]\n\u001b[32m    636\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ds/pro/smart-meeting-copilot/smartcopilot-api/.venv/lib/python3.12/site-packages/langgraph/graph/graph.py:289\u001b[39m, in \u001b[36mGraph.validate\u001b[39m\u001b[34m(self, interrupt)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m branch.ends \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m end \u001b[38;5;129;01min\u001b[39;00m branch.ends.values():\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m end != END:\n\u001b[32m    290\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    291\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAt \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m node, \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcond\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m branch found unknown target \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m             )\n\u001b[32m    293\u001b[39m         all_targets.add(end)\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "import random\n",
    "import time\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Keep the original simple state - just add what's needed\n",
    "class GraphState(TypedDict):\n",
    "    document_content: str\n",
    "    summary_status: str\n",
    "    insights_status: str\n",
    "    summary_output: str\n",
    "    insights_output: str\n",
    "    iteration: int\n",
    "    error_message: str\n",
    "    current_reasoning: str\n",
    "\n",
    "# Simple but intelligent decision model\n",
    "class SupervisorDecision(BaseModel):\n",
    "    \"\"\"AI Supervisor decision with intelligent reasoning\"\"\"\n",
    "    next_action: Literal[\n",
    "        \"call_both_parallel\", \n",
    "        \"call_summary_only\", \n",
    "        \"call_insights_only\", \n",
    "        \"end_workflow\"\n",
    "    ] = Field(description=\"What to do next in the workflow\")\n",
    "    \n",
    "    reasoning: str = Field(description=\"Why this decision makes sense for the workflow goal\")\n",
    "    \n",
    "    confidence: float = Field(\n",
    "        description=\"Confidence in this decision (0.0 to 1.0)\", \n",
    "        ge=0.0, le=1.0\n",
    "    )\n",
    "\n",
    "def intelligent_supervisor(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    Smart supervisor that understands the workflow goal:\n",
    "    - Get meeting minutes summary \n",
    "    - Get key insights in parallel\n",
    "    - Be intelligent about when to retry vs when to finish\n",
    "    \"\"\"\n",
    "    print(f\"\\n🧠 AI Supervisor thinking... (Iteration: {state['iteration']})\")\n",
    "    \n",
    "    # System prompt focused on workflow intelligence, not error handling\n",
    "    system_prompt = \"\"\"\n",
    "    You are an intelligent workflow supervisor. Your ONLY goal is to ensure we get:\n",
    "    1. A good summary of the meeting minutes\n",
    "    2. Key insights from the meeting minutes  \n",
    "    3. Both should be done efficiently\n",
    "    \n",
    "    You need to be SMART about:\n",
    "    - When something failed due to temporary issues (network, API busy) → retry makes sense\n",
    "    - When we have what we need → finish the workflow  \n",
    "    - When to run agents in parallel vs individually based on current state\n",
    "    - When we've tried enough and should accept current results\n",
    "    \n",
    "    WORKFLOW CONTEXT:\n",
    "    - Meeting document: \"{doc_preview}\"\n",
    "    - Summary status: {summary_status} \n",
    "    - Insights status: {insights_status}\n",
    "    - Current iteration: {iteration}\n",
    "    - Last error: {error_msg}\n",
    "    \n",
    "    THINK INTELLIGENTLY:\n",
    "    - If both are successful → we're done!\n",
    "    - If it's iteration 1 → start both in parallel (efficient)\n",
    "    - If one failed and other succeeded → retry just the failed one\n",
    "    - If both failed but iteration is low → try both again (might be temporary issue)\n",
    "    - If we've tried many times → maybe accept partial results or finish\n",
    "    - Consider: is this a workflow issue or just temporary network/API hiccup?\n",
    "    \n",
    "    Be smart, not just rule-based. Think about the actual goal: getting meeting summary + insights.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format the prompt with current state\n",
    "    doc_preview = state['document_content'][:100] + \"...\" if len(state['document_content']) > 100 else state['document_content']\n",
    "    \n",
    "    formatted_prompt = system_prompt.format(\n",
    "        doc_preview=doc_preview,\n",
    "        summary_status=state['summary_status'],\n",
    "        insights_status=state['insights_status'], \n",
    "        iteration=state['iteration'],\n",
    "        error_msg=state['error_message'] or \"None\"\n",
    "    )\n",
    "    \n",
    "    # TODO: Replace with actual LLM call\n",
    "    \"\"\"\n",
    "    # With Qwen or other reasoning model:\n",
    "    from langchain_community.llms import Ollama\n",
    "    llm = Ollama(model=\"qwen2.5:14b\")  # or qwen2.5:32b for even better reasoning\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=formatted_prompt),\n",
    "        HumanMessage(content=\"Analyze the workflow state and decide what to do next. Think step by step about our goal: meeting summary + insights.\")\n",
    "    ]\n",
    "    \n",
    "    decision = llm.with_structured_output(SupervisorDecision).invoke(messages)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate intelligent decision (replace with real LLM)\n",
    "    decision = simulate_smart_decision(state)\n",
    "    \n",
    "    print(f\"💭 AI Reasoning: {decision.reasoning}\")\n",
    "    print(f\"⚡ Decision: {decision.next_action} (confidence: {decision.confidence:.2f})\")\n",
    "    \n",
    "    # Update state with AI reasoning\n",
    "    new_state = state.copy()\n",
    "    new_state['current_reasoning'] = decision.reasoning\n",
    "    \n",
    "    return decision.next_action if decision.next_action != \"end_workflow\" else END\n",
    "\n",
    "\n",
    "\n",
    "def simulate_smart_decision(state: GraphState) -> SupervisorDecision:\n",
    "    \"\"\"\n",
    "    Simulate what an intelligent model like Qwen would decide\n",
    "    This is just simulation - replace with actual LLM call\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_status = state['summary_status']\n",
    "    insights_status = state['insights_status']\n",
    "    iteration = state['iteration']\n",
    "    \n",
    "    # Smart decision making that a good reasoning model would do\n",
    "    \n",
    "    # Goal achieved - both successful\n",
    "    if summary_status == \"success\" and insights_status == \"success\":\n",
    "        return SupervisorDecision(\n",
    "            next_action=\"end_workflow\",\n",
    "            reasoning=\"Perfect! Both summary and insights are ready. Our workflow goal is complete - we have meeting summary + key insights as requested.\",\n",
    "            confidence=1.0\n",
    "        )\n",
    "    \n",
    "    # First iteration - start efficiently  \n",
    "    if iteration == 1 and summary_status == \"pending\" and insights_status == \"pending\":\n",
    "        return SupervisorDecision(\n",
    "            next_action=\"call_both_parallel\", \n",
    "            reasoning=\"First attempt - running both summary and insights agents in parallel for efficiency. This is the optimal starting strategy.\",\n",
    "            confidence=0.9\n",
    "        )\n",
    "    \n",
    "    # One succeeded, one failed - targeted retry\n",
    "    if summary_status == \"success\" and insights_status == \"failed\":\n",
    "        if iteration <= 3:  # Smart about retry limits\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"call_insights_only\",\n",
    "                reasoning=\"Summary is ready, but insights failed. Retrying only insights agent since summary is already successful. Efficient targeted approach.\",\n",
    "                confidence=0.8\n",
    "            )\n",
    "        else:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"end_workflow\",\n",
    "                reasoning=\"Summary is ready and we've tried insights multiple times. Sometimes partial success is acceptable for meeting processing workflow.\",\n",
    "                confidence=0.6\n",
    "            )\n",
    "    \n",
    "    if insights_status == \"success\" and summary_status == \"failed\":\n",
    "        if iteration <= 3:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"call_summary_only\", \n",
    "                reasoning=\"Insights are ready, but summary failed. Retrying only summary agent since insights are already successful. Focused retry approach.\",\n",
    "                confidence=0.8\n",
    "            )\n",
    "        else:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"end_workflow\",\n",
    "                reasoning=\"Insights are ready and we've tried summary multiple times. We have key insights from the meeting which provides value.\",\n",
    "                confidence=0.6\n",
    "            )\n",
    "    \n",
    "    # Both failed - intelligent retry decision\n",
    "    if summary_status == \"failed\" and insights_status == \"failed\":\n",
    "        if iteration <= 2:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"call_both_parallel\",\n",
    "                reasoning=\"Both agents failed, but it's early in the process. Likely a temporary issue (network/API). Retrying both in parallel - efficient recovery approach.\",\n",
    "                confidence=0.7\n",
    "            )\n",
    "        elif iteration <= 4:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"call_both_parallel\", \n",
    "                reasoning=\"Multiple failures but still within reasonable retry range. The meeting document seems valid, so this might be temporary service issues. One more parallel attempt.\",\n",
    "                confidence=0.5\n",
    "            )\n",
    "        else:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"end_workflow\",\n",
    "                reasoning=\"After multiple attempts, continuing may not be productive. This could be a deeper issue with the document format or service availability. Ending workflow.\",\n",
    "                confidence=0.8\n",
    "            )\n",
    "    \n",
    "    # Default intelligent fallback\n",
    "    return SupervisorDecision(\n",
    "        next_action=\"call_both_parallel\",\n",
    "        reasoning=\"Current state requires both agents to run. Taking parallel approach for efficiency in meeting processing workflow.\", \n",
    "        confidence=0.6\n",
    "    )\n",
    "\n",
    "# Keep your original agent functions - they're fine\n",
    "def run_summary_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"Summary agent - focused on meeting summary\"\"\"\n",
    "    print(f\"\\n📝 Summary Agent working on meeting minutes...\")\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # Simulate work with some intelligence about content\n",
    "    success_rate = 0.7 if \"meeting\" in state['document_content'].lower() else 0.5\n",
    "    \n",
    "    if random.random() < success_rate:\n",
    "        new_state['summary_status'] = \"success\"\n",
    "        new_state['summary_output'] = f\"Meeting Summary: Key decisions and action items extracted from meeting minutes (iteration {state['iteration']})\"\n",
    "        print(\"✅ Summary: Generated meeting summary successfully\")\n",
    "    else:\n",
    "        new_state['summary_status'] = \"failed\" \n",
    "        new_state['summary_output'] = \"Summary generation failed\"\n",
    "        new_state['error_message'] = \"Temporary API issue during summary generation\"\n",
    "        print(\"❌ Summary: Failed (likely temporary issue)\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    return new_state\n",
    "\n",
    "def run_insights_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"Insights agent - focused on meeting insights\"\"\"\n",
    "    print(f\"\\n🔍 Insights Agent extracting key insights...\")\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # Simulate work with some intelligence about content  \n",
    "    success_rate = 0.7 if \"meeting\" in state['document_content'].lower() else 0.5\n",
    "    \n",
    "    if random.random() < success_rate:\n",
    "        new_state['insights_status'] = \"success\"\n",
    "        new_state['insights_output'] = f\"Meeting Insights: Key themes, decisions, and follow-ups identified (iteration {state['iteration']})\"\n",
    "        print(\"✅ Insights: Extracted meeting insights successfully\")\n",
    "    else:\n",
    "        new_state['insights_status'] = \"failed\"\n",
    "        new_state['insights_output'] = \"Insights generation failed\"  \n",
    "        new_state['error_message'] = \"Temporary processing issue during insights extraction\"\n",
    "        print(\"❌ Insights: Failed (likely temporary issue)\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    return new_state\n",
    "\n",
    "def increment_iteration(state: GraphState) -> GraphState:\n",
    "    \"\"\"Track iterations for intelligent decision making\"\"\"\n",
    "    new_state = state.copy()\n",
    "    new_state['iteration'] += 1\n",
    "    print(f\"\\n--- Workflow Iteration: {new_state['iteration']} ---\")\n",
    "    return new_state\n",
    "\n",
    "# Build the workflow graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"increment_iteration\", increment_iteration)\n",
    "workflow.add_node(\"intelligent_supervisor\", intelligent_supervisor)  # This is the key AI decision maker\n",
    "workflow.add_node(\"run_summary_agent\", run_summary_agent)\n",
    "workflow.add_node(\"run_insights_agent\", run_insights_agent)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"increment_iteration\")\n",
    "\n",
    "# Define flow\n",
    "workflow.add_edge(\"increment_iteration\", \"intelligent_supervisor\")\n",
    "\n",
    "# The AI supervisor makes intelligent routing decisions\n",
    "workflow.add_conditional_edges(\n",
    "    \"intelligent_supervisor\",\n",
    "    intelligent_supervisor,\n",
    "    {\n",
    "        \"call_both_parallel\": [\"run_summary_agent\", \"run_insights_agent\"],\n",
    "        \"call_summary_only\": \"run_summary_agent\", \n",
    "        \"call_insights_only\": \"run_insights_agent\",\n",
    "        \"end_workflow\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# After each agent, go back to supervisor for next intelligent decision\n",
    "workflow.add_edge(\"run_summary_agent\", \"intelligent_supervisor\")\n",
    "workflow.add_edge(\"run_insights_agent\", \"intelligent_supervisor\")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test the intelligent workflow\n",
    "print(\"🚀 Starting Intelligent Meeting Processing Workflow\")\n",
    "\n",
    "initial_state = {\n",
    "    \"document_content\": \"Meeting minutes from Q3 planning: Budget discussion, marketing strategy changes, team restructuring decisions, action items for next quarter...\",\n",
    "    \"summary_status\": \"pending\",\n",
    "    \"insights_status\": \"pending\", \n",
    "    \"summary_output\": \"\",\n",
    "    \"insights_output\": \"\",\n",
    "    \"iteration\": 0,\n",
    "    \"error_message\": \"\",\n",
    "    \"current_reasoning\": \"\"\n",
    "}\n",
    "\n",
    "# Run and see the AI make intelligent decisions\n",
    "for step in app.stream(initial_state):\n",
    "    if '__end__' not in step:\n",
    "        print(f\"\\n📊 Current State: {step}\")\n",
    "\n",
    "print(\"\\n🎯 Workflow completed with AI supervision!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13d26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a4d4347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b9fd8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8134d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ea4696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep the original simple state - just add what's needed\n",
    "class GraphState(TypedDict):\n",
    "    document_content: str\n",
    "    summary_status: str\n",
    "    insights_status: str\n",
    "    summary_output: str\n",
    "    insights_output: str\n",
    "    iteration: int\n",
    "    error_message: str\n",
    "    current_reasoning: str\n",
    "    next : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a560b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple but intelligent decision model\n",
    "class SupervisorDecision(BaseModel):\n",
    "    \"\"\"AI Supervisor decision with intelligent reasoning\"\"\"\n",
    "    next_action: Literal[\n",
    "        \"call_both_parallel\", \n",
    "        \"call_summary_only\", \n",
    "        \"call_insights_only\", \n",
    "        \"end_workflow\"\n",
    "    ] = Field(description=\"What to do next in the workflow\")\n",
    "    \n",
    "    reasoning: str = Field(description=\"Why this decision makes sense for the workflow goal\")\n",
    "    \n",
    "    confidence: float = Field(\n",
    "        description=\"Confidence in this decision (0.0 to 1.0)\", \n",
    "        ge=0.0, le=1.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c1af35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "def intelligent_supervisor(state: GraphState) ->  Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Smart supervisor that understands the workflow goal:\n",
    "    - Get meeting minutes summary \n",
    "    - Get key insights in parallel\n",
    "    - Be intelligent about when to retry vs when to finish\n",
    "    \"\"\"\n",
    "    print(f\"\\n🧠 AI Supervisor thinking... (Iteration: {state['iteration']})\")\n",
    "    \n",
    "    # Enhanced ReAct-style system prompt\n",
    "    system_prompt = \"\"\"\n",
    "    You are an intelligent workflow supervisor using ReAct (Reasoning + Acting) methodology. \n",
    "\n",
    "    ## YOUR MISSION:\n",
    "    Ensure we successfully get:\n",
    "    1. A good SUMMARY of the meeting minutes\n",
    "    2. KEY INSIGHTS from the meeting minutes\n",
    "    3. Both delivered EFFICIENTLY\n",
    "\n",
    "    ## ReAct PROCESS - Think step by step:\n",
    "\n",
    "    **THOUGHT**: First, analyze the current workflow state. What's working? What failed? Why might it have failed?\n",
    "\n",
    "    **OBSERVATION**: What do you observe about the current status? Look at:\n",
    "    - What agents have succeeded/failed\n",
    "    - How many iterations we've done  \n",
    "    - Whether failures seem temporary (network/API issues) or persistent\n",
    "    - If we have partial success that might be sufficient\n",
    "\n",
    "    **ACTION**: Based on your thought and observation, decide the smartest next action for our meeting processing goal.\n",
    "\n",
    "    ## CURRENT WORKFLOW STATE:\n",
    "    - Meeting Document: \"{doc_preview}\"\n",
    "    - Summary Status: {summary_status}\n",
    "    - Insights Status: {insights_status} \n",
    "    - Current Iteration: {iteration}\n",
    "    - Last Error: {error_msg}\n",
    "\n",
    "    ## AVAILABLE ACTIONS:\n",
    "    - call_both_parallel: Run both agents simultaneously (efficient for fresh start or when both need work)\n",
    "    - call_summary_only: Focus only on getting the meeting summary\n",
    "    - call_insights_only: Focus only on getting meeting insights  \n",
    "    - end_workflow: We have achieved our goal (both summary + insights ready)\n",
    "\n",
    "    ## INTELLIGENT DECISION GUIDELINES:\n",
    "    - Iteration 1: Usually start with parallel execution for efficiency\n",
    "    - One success, one failure: Target retry the failed agent only\n",
    "    - Both failed early iterations: Likely temporary issues, retry both\n",
    "    - Multiple failures: Consider if we should accept partial results or continue\n",
    "    - Both successful: Mission accomplished!\n",
    "\n",
    "    **Remember**: You're optimizing for getting useful meeting analysis (summary + insights), not perfect success rates.\n",
    "\n",
    "    Use the ReAct process: THOUGHT → OBSERVATION → ACTION with clear reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format the prompt with current state\n",
    "    doc_preview = state['document_content'][:100] + \"...\" if len(state['document_content']) > 100 else state['document_content']\n",
    "    \n",
    "    formatted_prompt = system_prompt.format(\n",
    "        doc_preview=doc_preview,\n",
    "        summary_status=state['summary_status'],\n",
    "        insights_status=state['insights_status'], \n",
    "        iteration=state['iteration'],\n",
    "        error_msg=state['error_message'] or \"None\"\n",
    "    )\n",
    "    \n",
    "    # Setup Groq API with Qwen model\n",
    "    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not groq_api_key:\n",
    "        raise ValueError(\"GROQ_API_KEY environment variable is not set\")\n",
    "    \n",
    "    os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
    "    \n",
    "    llm = ChatGroq(\n",
    "        temperature=0,\n",
    "        model=\"deepseek-r1-distill-llama-70b\"  \n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=formatted_prompt),\n",
    "        HumanMessage(content=\"Use ReAct methodology: THOUGHT → OBSERVATION → ACTION. Analyze the workflow state and decide what to do next for our meeting processing goal. Think step by step.\")\n",
    "    ]\n",
    "    \n",
    "    # Get structured decision from LLM\n",
    "    try:\n",
    "        decision = llm.with_structured_output(SupervisorDecision).invoke(messages)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ LLM call failed: {e}\")\n",
    "        # Fallback to simulation if LLM fails\n",
    "        decision = simulate_smart_decision(state)\n",
    "    \n",
    "    # # Simulate intelligent decision (replace with real LLM)\n",
    "    # decision = simulate_smart_decision(state)\n",
    "    \n",
    "    print(f\"💭 AI Reasoning: {decision.reasoning}\")\n",
    "    print(f\"⚡ Decision: {decision.next_action} (confidence: {decision.confidence:.2f})\")\n",
    "    \n",
    "    # # Update state with AI reasoning\n",
    "    # new_state = state.copy()\n",
    "    # new_state['next'] = decision.next_action  # Store the next action\n",
    "    # new_state['current_reasoning'] = decision.reasoning\n",
    "    \n",
    "    # return decision.next_action if decision.next_action != \"end_workflow\" else END\n",
    "    goto = decision.next_action if decision.next_action != \"end_workflow\" else END\n",
    "\n",
    "    print('goto:',goto)\n",
    "\n",
    "    return {\n",
    "        \"current_reasoning\": decision.reasoning,\n",
    "        \"next\": decision.next_action # Store the determined next action\n",
    "    }\n",
    "\n",
    "    # return Command(\n",
    "    #         # goto=goto if goto != \"FINISH\" else \"__end__\",\n",
    "    #         goto = goto,\n",
    "    #         update={\n",
    "    #             \"next\": goto,\n",
    "                \n",
    "    #             \"current_reasoning\": decision.reasoning,\n",
    "    #             # \"messages\": updated_messages  # Use updated messages\n",
    "    #         }\n",
    "    #     ) # pyright: ignore[reportReturnType]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38bc4e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def simulate_smart_decision(state: GraphState) -> SupervisorDecision:\n",
    "    \"\"\"\n",
    "    Simulate what an intelligent model like Qwen would decide\n",
    "    This is just simulation - replace with actual LLM call\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_status = state['summary_status']\n",
    "    insights_status = state['insights_status']\n",
    "    iteration = state['iteration']\n",
    "    \n",
    "    # Smart decision making that a good reasoning model would do\n",
    "    \n",
    "    # Goal achieved - both successful\n",
    "    if summary_status == \"success\" and insights_status == \"success\":\n",
    "        return SupervisorDecision(\n",
    "            next_action=\"end_workflow\",\n",
    "            reasoning=\"Perfect! Both summary and insights are ready. Our workflow goal is complete - we have meeting summary + key insights as requested.\",\n",
    "            confidence=1.0\n",
    "        )\n",
    "    \n",
    "    # First iteration - start efficiently  \n",
    "    if iteration == 1 and summary_status == \"pending\" and insights_status == \"pending\":\n",
    "        return SupervisorDecision(\n",
    "            next_action=\"call_both_parallel\", \n",
    "            reasoning=\"First attempt - running both summary and insights agents in parallel for efficiency. This is the optimal starting strategy.\",\n",
    "            confidence=0.9\n",
    "        )\n",
    "    \n",
    "    # One succeeded, one failed - targeted retry\n",
    "    if summary_status == \"success\" and insights_status == \"failed\":\n",
    "        if iteration <= 3:  # Smart about retry limits\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"call_insights_only\",\n",
    "                reasoning=\"Summary is ready, but insights failed. Retrying only insights agent since summary is already successful. Efficient targeted approach.\",\n",
    "                confidence=0.8\n",
    "            )\n",
    "        else:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"end_workflow\",\n",
    "                reasoning=\"Summary is ready and we've tried insights multiple times. Sometimes partial success is acceptable for meeting processing workflow.\",\n",
    "                confidence=0.6\n",
    "            )\n",
    "    \n",
    "    if insights_status == \"success\" and summary_status == \"failed\":\n",
    "        if iteration <= 3:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"call_summary_only\", \n",
    "                reasoning=\"Insights are ready, but summary failed. Retrying only summary agent since insights are already successful. Focused retry approach.\",\n",
    "                confidence=0.8\n",
    "            )\n",
    "        else:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"end_workflow\",\n",
    "                reasoning=\"Insights are ready and we've tried summary multiple times. We have key insights from the meeting which provides value.\",\n",
    "                confidence=0.6\n",
    "            )\n",
    "    \n",
    "    # Both failed - intelligent retry decision\n",
    "    if summary_status == \"failed\" and insights_status == \"failed\":\n",
    "        if iteration <= 2:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"call_both_parallel\",\n",
    "                reasoning=\"Both agents failed, but it's early in the process. Likely a temporary issue (network/API). Retrying both in parallel - efficient recovery approach.\",\n",
    "                confidence=0.7\n",
    "            )\n",
    "        elif iteration <= 4:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"call_both_parallel\", \n",
    "                reasoning=\"Multiple failures but still within reasonable retry range. The meeting document seems valid, so this might be temporary service issues. One more parallel attempt.\",\n",
    "                confidence=0.5\n",
    "            )\n",
    "        else:\n",
    "            return SupervisorDecision(\n",
    "                next_action=\"end_workflow\",\n",
    "                reasoning=\"After multiple attempts, continuing may not be productive. This could be a deeper issue with the document format or service availability. Ending workflow.\",\n",
    "                confidence=0.8\n",
    "            )\n",
    "    \n",
    "    # Default intelligent fallback\n",
    "    return SupervisorDecision(\n",
    "        next_action=\"call_both_parallel\",\n",
    "        reasoning=\"Current state requires both agents to run. Taking parallel approach for efficiency in meeting processing workflow.\", \n",
    "        confidence=0.6\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "684ebd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_supervisor_decision(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    Routes the workflow based on the decision stored by the intelligent supervisor.\n",
    "    \"\"\"\n",
    "    return state['next'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4ffa5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep your original agent functions - they're fine\n",
    "def run_summary_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"Summary agent - focused on meeting summary\"\"\"\n",
    "    print(f\"\\n📝 Summary Agent working on meeting minutes...\")\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # Simulate work with some intelligence about content\n",
    "    success_rate = 0.1 if \"meeting\" in state['document_content'].lower() else 0.3\n",
    "    \n",
    "    if random.random() < success_rate:\n",
    "        new_state['summary_status'] = \"success\"\n",
    "        new_state['summary_output'] = f\"Meeting Summary: Key decisions and action items extracted from meeting minutes (iteration {state['iteration']})\"\n",
    "        print(\"✅ Summary: Generated meeting summary successfully\")\n",
    "    else:\n",
    "        new_state['summary_status'] = \"failed\" \n",
    "        new_state['summary_output'] = \"Summary generation failed\"\n",
    "        new_state['error_message'] = \"Temporary API issue during summary generation\"\n",
    "        print(\"❌ Summary: Failed (likely temporary issue)\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    return new_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08f9de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_insights_agent(state: GraphState) -> GraphState:\n",
    "    \"\"\"Insights agent - focused on meeting insights\"\"\"\n",
    "    print(f\"\\n🔍 Insights Agent extracting key insights...\")\n",
    "    new_state = state.copy()\n",
    "    \n",
    "    # Simulate work with some intelligence about content  \n",
    "    success_rate = 0.5 if \"meeting\" in state['document_content'].lower() else 0.3\n",
    "    \n",
    "    if random.random() < success_rate:\n",
    "        new_state['insights_status'] = \"success\"\n",
    "        new_state['insights_output'] = f\"Meeting Insights: Key themes, decisions, and follow-ups identified (iteration {state['iteration']})\"\n",
    "        print(\"✅ Insights: Extracted meeting insights successfully\")\n",
    "    else:\n",
    "        new_state['insights_status'] = \"failed\"\n",
    "        new_state['insights_output'] = \"Insights generation failed\"  \n",
    "        new_state['error_message'] = \"Temporary processing issue during insights extraction\"\n",
    "        print(\"❌ Insights: Failed (likely temporary issue)\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57f5d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "def run_both_parallel_agents(state: GraphState) -> dict:\n",
    "    print(\"---RUNNING BOTH AGENTS IN PARALLEL---\")\n",
    "\n",
    "    # Define the parallel execution block\n",
    "    # Each key in the dict defines the output key for RunnableParallel\n",
    "    # The value is a Runnable that receives the original input (state)\n",
    "    parallel_runnable = RunnableParallel(\n",
    "        summary_agent_output=run_summary_agent,\n",
    "        insights_agent_output=run_insights_agent\n",
    "    )\n",
    "\n",
    "    # Execute the parallel runnables with the current state as input\n",
    "    parallel_results = parallel_runnable.invoke(state)\n",
    "\n",
    "    # Extract results and statuses from the parallel_results\n",
    "    # Note: run_summary_agent returns {\"summary_result\": ..., \"summary_status\": ...}\n",
    "    # So parallel_results['summary_agent_output'] will be that dict.\n",
    "    summary_result = parallel_results['summary_agent_output'].get('summary_output')\n",
    "    summary_status = parallel_results['summary_agent_output'].get('summary_status')\n",
    "    insights_result = parallel_results['insights_agent_output'].get('insights_output')\n",
    "    insights_status = parallel_results['insights_agent_output'].get('insights_status')\n",
    "\n",
    "    # Return a dictionary to update the graph state\n",
    "    return {\n",
    "        \"summary_output\": summary_result,\n",
    "        \"summary_status\": summary_status,\n",
    "        \"insights_output\": insights_result,\n",
    "        \"insights_status\": insights_status\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4373f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_iteration(state: GraphState) -> GraphState:\n",
    "    \"\"\"Track iterations for intelligent decision making\"\"\"\n",
    "    new_state = state.copy()\n",
    "    new_state['iteration'] += 1\n",
    "    print(f\"\\n--- Workflow Iteration: {new_state['iteration']} ---\")\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c1b789c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7c199c3553d0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the workflow graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"increment_iteration\", increment_iteration)\n",
    "workflow.add_node(\"intelligent_supervisor\", intelligent_supervisor)\n",
    "workflow.add_node(\"run_summary_agent\", run_summary_agent)\n",
    "workflow.add_node(\"run_insights_agent\", run_insights_agent)\n",
    "workflow.add_node(\"run_parallel_agents\", run_both_parallel_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "46e59386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7c199c3553d0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"increment_iteration\")\n",
    "\n",
    "# Define initial edge from entry point to supervisor\n",
    "workflow.add_edge(\"increment_iteration\", \"intelligent_supervisor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4bad92f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7c199c3553d0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The AI supervisor makes intelligent routing decisions\n",
    "# - The edge starts from the 'intelligent_supervisor' node.\n",
    "# - The 'route_supervisor_decision' function defines how to choose the next path.\n",
    "workflow.add_conditional_edges(\n",
    "    \"intelligent_supervisor\",     # Conditional edges originate from the 'intelligent_supervisor' node\n",
    "    route_supervisor_decision,    # This function defines the routing logic (returns the key for the dict)\n",
    "    {\n",
    "        \"call_both_parallel\": \"run_parallel_agents\", # Map decision to the parallel execution node\n",
    "        \"call_insights_only\": \"run_insights_agent\",  # Map decision to the insights agent node\n",
    "        \"call_summary_only\": \"run_summary_agent\",    # Map decision to the summary agent node\n",
    "        \"end_workflow\": END                          # Map decision to the END state\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ebdc0e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7c199c3553d0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After each agent completes, return to the supervisor for the next intelligent decision\n",
    "workflow.add_edge(\"run_summary_agent\", \"intelligent_supervisor\")\n",
    "workflow.add_edge(\"run_insights_agent\", \"intelligent_supervisor\")\n",
    "workflow.add_edge(\"run_parallel_agents\", \"intelligent_supervisor\") # Add this edge for parallel execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a0d9a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the workflow\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c3f2314c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Intelligent Meeting Processing Workflow\n",
      "\n",
      "--- Workflow Iteration: 1 ---\n",
      "\n",
      "📊 Current State: {'increment_iteration': {'document_content': 'Meeting minutes from Q3 planning: Budget discussion, marketing strategy changes, team restructuring decisions, action items for next quarter...', 'summary_status': 'pending', 'insights_status': 'pending', 'summary_output': '', 'insights_output': '', 'iteration': 1, 'error_message': '', 'current_reasoning': ''}}\n",
      "\n",
      "🧠 AI Supervisor thinking... (Iteration: 1)\n",
      "💭 AI Reasoning: Starting with parallel execution in the first iteration is efficient and aligns with the goal of obtaining both summary and insights promptly.\n",
      "⚡ Decision: call_both_parallel (confidence: 1.00)\n",
      "goto: call_both_parallel\n",
      "\n",
      "📊 Current State: {'intelligent_supervisor': {'current_reasoning': 'Starting with parallel execution in the first iteration is efficient and aligns with the goal of obtaining both summary and insights promptly.', 'next': 'call_both_parallel'}}\n",
      "---RUNNING BOTH AGENTS IN PARALLEL---\n",
      "\n",
      "📝 Summary Agent working on meeting minutes...\n",
      "❌ Summary: Failed (likely temporary issue)\n",
      "\n",
      "🔍 Insights Agent extracting key insights...\n",
      "✅ Insights: Extracted meeting insights successfully\n",
      "\n",
      "📊 Current State: {'run_parallel_agents': {'summary_output': 'Summary generation failed', 'summary_status': 'failed', 'insights_output': 'Meeting Insights: Key themes, decisions, and follow-ups identified (iteration 1)', 'insights_status': 'success'}}\n",
      "\n",
      "🧠 AI Supervisor thinking... (Iteration: 1)\n",
      "💭 AI Reasoning: Summary failed while insights succeeded in the first iteration. Targeting the failed component to achieve both goals efficiently.\n",
      "⚡ Decision: call_summary_only (confidence: 0.90)\n",
      "goto: call_summary_only\n",
      "\n",
      "📊 Current State: {'intelligent_supervisor': {'current_reasoning': 'Summary failed while insights succeeded in the first iteration. Targeting the failed component to achieve both goals efficiently.', 'next': 'call_summary_only'}}\n",
      "\n",
      "📝 Summary Agent working on meeting minutes...\n",
      "✅ Summary: Generated meeting summary successfully\n",
      "\n",
      "📊 Current State: {'run_summary_agent': {'document_content': 'Meeting minutes from Q3 planning: Budget discussion, marketing strategy changes, team restructuring decisions, action items for next quarter...', 'summary_status': 'success', 'insights_status': 'success', 'summary_output': 'Meeting Summary: Key decisions and action items extracted from meeting minutes (iteration 1)', 'insights_output': 'Meeting Insights: Key themes, decisions, and follow-ups identified (iteration 1)', 'iteration': 1, 'error_message': '', 'current_reasoning': 'Summary failed while insights succeeded in the first iteration. Targeting the failed component to achieve both goals efficiently.', 'next': 'call_summary_only'}}\n",
      "\n",
      "🧠 AI Supervisor thinking... (Iteration: 1)\n",
      "💭 AI Reasoning: Both the summary and insights were successfully generated in the first iteration without any errors. Therefore, the workflow has achieved its goal and should be terminated.\n",
      "⚡ Decision: end_workflow (confidence: 1.00)\n",
      "goto: __end__\n",
      "\n",
      "📊 Current State: {'intelligent_supervisor': {'current_reasoning': 'Both the summary and insights were successfully generated in the first iteration without any errors. Therefore, the workflow has achieved its goal and should be terminated.', 'next': 'end_workflow'}}\n",
      "\n",
      "🎯 Workflow completed with AI supervision!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the intelligent workflow\n",
    "print(\"🚀 Starting Intelligent Meeting Processing Workflow\")\n",
    "\n",
    "initial_state = {\n",
    "    \"document_content\": \"Meeting minutes from Q3 planning: Budget discussion, marketing strategy changes, team restructuring decisions, action items for next quarter...\",\n",
    "    \"summary_status\": \"pending\",\n",
    "    \"insights_status\": \"pending\", \n",
    "    \"summary_output\": \"\",\n",
    "    \"insights_output\": \"\",\n",
    "    \"iteration\": 0,\n",
    "    \"error_message\": \"\",\n",
    "    \"current_reasoning\": \"\"\n",
    "}\n",
    "\n",
    "# Run and see the AI make intelligent decisions\n",
    "for step in app.stream(initial_state):\n",
    "    if '__end__' not in step:\n",
    "        print(f\"\\n📊 Current State: {step}\")\n",
    "\n",
    "print(\"\\n🎯 Workflow completed with AI supervision!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43304e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartcopilot-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
